{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* creates `deriv/recall_df.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import brainiak\n",
    "import nilearn as nl\n",
    "\n",
    "from nilearn import image, plotting, input_data\n",
    "from scipy.spatial import distance\n",
    "\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df = pd.read_csv('deriv/view_df.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make the initial logfile df with onsetTR, offsetTR and wed_id - 12 rows, one per wedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mylogfile(sub_num):\n",
    "  \"\"\" \n",
    "  read logfile into dataframe\n",
    "    NB using a different logfile from the one used in view\n",
    "  returns df with 12 rows: each row recall cue\n",
    "  \"\"\"\n",
    "  TR_rate = 1.5\n",
    "  # load sub log file\n",
    "  fpath = 'data/behav/silvy_buckets/sub%.2iday2/%i_recall_mylog.log'%(100+sub_num,sub_num)\n",
    "  f = open(fpath, \"r\")\n",
    "  first_line = f.readline()\n",
    "  # extract first TR tstamp\n",
    "  first_tstamp = float(first_line.split()[1])\n",
    "  # init df\n",
    "  sub_df = pd.DataFrame(columns=['sub_num','wed_id','onsetTR'])\n",
    "  # loop over rows in logfile (stim onsets)\n",
    "  for x in f:\n",
    "    # extract info\n",
    "    RV,tstamp,stim = x.split(' ')\n",
    "    tstamp = float(tstamp[:-1])\n",
    "    wed_id = stim.split('/')[-1].split('.')[0][1:]\n",
    "    onsetTR = np.floor((tstamp-first_tstamp)/TR_rate)\n",
    "    # populate df with info\n",
    "    sub_df.loc[tstamp,'sub_num'] = sub_num\n",
    "    sub_df.loc[tstamp,'wed_id'] = wed_id\n",
    "    sub_df.loc[tstamp,'onsetTR'] = onsetTR \n",
    "  return sub_df\n",
    "\n",
    "\n",
    "def get_final_recall_TR(sub_num):\n",
    "  \"\"\"\n",
    "  currently using nuisance var file to find last TR\n",
    "  NB roi files have extra TR, so I adjust\n",
    "  \"\"\"\n",
    "  final_TR = pd.read_csv(\n",
    "    \"data/fmri/selected_nuisance/sub-%i_ses-02_task-recall_confounds_selected.txt\"%(100+sub_num)).shape[0]\n",
    "  # adjust for extra TR in ROi files compared to nuisance\n",
    "  final_TR += 1 \n",
    "  return final_TR\n",
    "\n",
    "\n",
    "def include_offsetTR_col(df):\n",
    "  \"\"\" set offset of even as onset of next event\n",
    "  \"\"\"\n",
    "  df = df.sort_values('onsetTR')\n",
    "  df.loc[:,'wed_num_recall'] = np.arange(12)\n",
    "  df.index = np.arange(12)\n",
    "  df.loc[np.arange(11),'offsetTR'] = df.iloc[1:].onsetTR.values\n",
    "  # final TR from roi file\n",
    "  sub_num = df.sub_num.unique()[0]\n",
    "  df.loc[11,'offsetTR'] = get_final_recall_TR(sub_num)\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from logfile_df make recall_df where each row is a TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_recall_df(logdf):\n",
    "  \"\"\"\n",
    "  expand logdf so that each row is a TR\n",
    "  \"\"\"\n",
    "  L = []\n",
    "  for idx,log_df_row in logdf.iterrows(): \n",
    "    for TR in np.arange(log_df_row.onsetTR,log_df_row.offsetTR):\n",
    "      D = {}\n",
    "      D['TR'] = int(TR)\n",
    "      D['sub_num'] = int(log_df_row.sub_num)\n",
    "      D['wed_id'] = int(log_df_row.wed_id)\n",
    "      D['wed_num_recall'] = int(log_df_row.wed_num_recall)\n",
    "      D['onsetTR'] = int(log_df_row.onsetTR)\n",
    "      D['offsetTR'] = int(log_df_row.offsetTR)\n",
    "      L.append(D)\n",
    "  return pd.DataFrame(L)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### include recall transcriptions for each TR of recall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcript_df(sub_num):\n",
    "  \"\"\" \n",
    "  reindex from seconds to TRs\n",
    "    NB this causes duplicates\n",
    "    to resolve duplicates, non-zero entries break the tie\n",
    "  \"\"\"\n",
    "  TR_rate = 1.5\n",
    "  tdf = pd.read_csv('data/behav/silvy_buckets/recallTranscriptions/S%i.csv'%sub_num,index_col=0).T.fillna(0)\n",
    "  if sub_num==6: \n",
    "    tdf = tdf.replace('\\n',0)   \n",
    "  # transform index from seconds to TRs\n",
    "  tdf.index = (tdf.index.astype(int)/TR_rate).astype(int)\n",
    "  tdf = tdf.astype(int)\n",
    "  return resolve_tdf_duplicates(tdf)\n",
    "\n",
    "def resolve_tdf_duplicates(tdf):\n",
    "  \"\"\"\n",
    "  when converting tdf seconds to TRs,\n",
    "    there are duplicates\n",
    "    to resolve duplicates, \n",
    "      take row with most non-zeros\n",
    "      *NB could be improved*\n",
    "  \"\"\"\n",
    "  L = []\n",
    "  for idx in tdf.index.unique():\n",
    "    rows = tdf.loc[idx,:]\n",
    "    # detect if there are duplicates\n",
    "    if len(rows.shape)==1:\n",
    "      row = rows\n",
    "    elif len(rows.shape)>1:\n",
    "      # resolve duplicate entries\n",
    "      keep_row_num = np.sum(rows != 0,1).argmax()    \n",
    "      row = rows.iloc[keep_row_num]\n",
    "    L.append(row)\n",
    "  return pd.concat(L,1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_recall_transcription(recall_df,transcript_df):\n",
    "  \"\"\" \n",
    "  using wedding_id to match between \n",
    "  information in log_df with transcriptions\n",
    "  \"\"\"\n",
    "  for idx,recall_df_row in recall_df.iterrows():\n",
    "    transcript_df_row_num = recall_df_row.TR - int(recall_df_row.onsetTR)\n",
    "    recall = transcript_df.loc[transcript_df_row_num,\"W%i\"%int(recall_df_row.wed_id)]\n",
    "    recall_df.loc[idx,'recall'] = int(recall)\n",
    "  recall_df = recall_df.astype({'recall':int})\n",
    "  return recall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop over subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sub_recall_df(sub_num):\n",
    "  log_df = read_mylogfile(sub_num)\n",
    "  log_df = include_offsetTR_col(log_df)\n",
    "  recall_df = init_recall_df(log_df)\n",
    "  ## include transcribed recall\n",
    "  transcript_df = load_transcript_df(sub_num)\n",
    "  transcript_df = resolve_tdf_duplicates(transcript_df)\n",
    "  recall_df = include_recall_transcription(recall_df,transcript_df)\n",
    "  return recall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err, sub 0\n",
      "err, sub 1\n",
      "err, sub 15\n",
      "err, sub 16\n",
      "err, sub 20\n",
      "err, sub 21\n"
     ]
    }
   ],
   "source": [
    "dfL = []\n",
    "for sub_num in np.arange(45):\n",
    "  try:\n",
    "    dfL.append(build_sub_recall_df(sub_num))\n",
    "  except:\n",
    "    print('err, sub',sub_num)\n",
    "    \n",
    "recall_df = pd.concat(dfL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean-up and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONCAT AND REIDNEX\n",
    "recall_df.index = np.arange(len(recall_df))\n",
    "recall_df = recall_df.astype({'TR':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_df.to_csv('deriv/recall_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
