{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* creates `deriv/recall_df.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import brainiak\n",
    "import nilearn as nl\n",
    "\n",
    "from nilearn import image, plotting, input_data\n",
    "from scipy.spatial import distance\n",
    "\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df = pd.read_csv('deriv/view_df.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make the initial logfile df with onsetTR, offsetTR and wed_id - 12 rows, one per wedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mylogfile(sub_num):\n",
    "  \"\"\" \n",
    "  read logfile into dataframe\n",
    "    NB using a different logfile from the one used in view\n",
    "  returns df with 12 rows: each row recall cue\n",
    "  \"\"\"\n",
    "  TR_rate = 1.5\n",
    "  # load sub log file\n",
    "  fpath = 'data/behav/silvy_buckets/sub%.2iday2/%i_recall_mylog.log'%(100+sub_num,sub_num)\n",
    "  f = open(fpath, \"r\")\n",
    "  first_line = f.readline()\n",
    "  # extract first TR tstamp\n",
    "  first_tstamp = float(first_line.split()[1])\n",
    "  # init df\n",
    "  sub_df = pd.DataFrame(columns=['sub_num','wed_id','onsetTR'])\n",
    "  # loop over rows in logfile (stim onsets)\n",
    "  for x in f:\n",
    "    # extract info\n",
    "    RV,tstamp,stim = x.split(' ')\n",
    "    tstamp = float(tstamp[:-1])\n",
    "    wed_id = stim.split('/')[-1].split('.')[0][1:]\n",
    "    onsetTR = np.floor((tstamp-first_tstamp)/TR_rate)\n",
    "    # populate df with info\n",
    "    sub_df.loc[tstamp,'sub_num'] = sub_num\n",
    "    sub_df.loc[tstamp,'wed_id'] = wed_id\n",
    "    sub_df.loc[tstamp,'onsetTR'] = onsetTR \n",
    "  return sub_df\n",
    "\n",
    "\n",
    "def get_final_recall_TR(sub_num):\n",
    "  \"\"\"\n",
    "  currently using nuisance var file to find last TR\n",
    "  NB roi files have extra TR, so I adjust\n",
    "  \"\"\"\n",
    "  final_TR = pd.read_csv(\n",
    "    \"data/fmri/selected_nuisance/sub-%i_ses-02_task-recall_confounds_selected.txt\"%(100+sub_num)).shape[0]\n",
    "  # adjust for extra TR in ROi files compared to nuisance\n",
    "  final_TR += 1 \n",
    "  return final_TR\n",
    "\n",
    "\n",
    "def include_offsetTR_col(df):\n",
    "  \"\"\" set offset of even as onset of next event\n",
    "  \"\"\"\n",
    "  df = df.sort_values('onsetTR')\n",
    "  df.loc[:,'wed_num_recall'] = np.arange(12)\n",
    "  df.index = np.arange(12)\n",
    "  df.loc[np.arange(11),'offsetTR'] = df.iloc[1:].onsetTR.values\n",
    "  # final TR from roi file\n",
    "  df.loc[11,'offsetTR'] = get_final_recall_TR(sub_num)\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from logfile_df make recall_df where each row is a TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_recall_df(logdf):\n",
    "  \"\"\"\n",
    "  expand logdf so that each row is a TR\n",
    "  \"\"\"\n",
    "  L = []\n",
    "  for idx,log_df_row in logdf.iterrows(): \n",
    "    for TR in np.arange(log_df_row.onsetTR,log_df_row.offsetTR):\n",
    "      D = {}\n",
    "      D['TR'] = int(TR)\n",
    "      D['sub_num'] = int(log_df_row.sub_num)\n",
    "      D['wed_id'] = int(log_df_row.wed_id)\n",
    "      D['wed_num_recall'] = int(log_df_row.wed_num_recall)\n",
    "      D['onsetTR'] = int(log_df_row.onsetTR)\n",
    "      D['offsetTR'] = int(log_df_row.offsetTR)\n",
    "      L.append(D)\n",
    "  return pd.DataFrame(L)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### include recall transcriptions for each TR of recall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcript_df(sub_num):\n",
    "  \"\"\" \n",
    "  reindex from seconds to TRs\n",
    "    NB this causes duplicates\n",
    "    to resolve duplicates, non-zero entries break the tie\n",
    "  \"\"\"\n",
    "  TR_rate = 1.5\n",
    "  tdf = pd.read_csv('data/behav/silvy_buckets/recallTranscriptions/S%i.csv'%sub_num,index_col=0).T.fillna(0)\n",
    "  if sub_num==6: \n",
    "    tdf = tdf.replace('\\n',0)   \n",
    "  # transform index from seconds to TRs\n",
    "  tdf.index = (tdf.index.astype(int)/TR_rate).astype(int)\n",
    "  tdf = tdf.astype(int)\n",
    "  return resolve_tdf_duplicates(tdf)\n",
    "\n",
    "def resolve_tdf_duplicates(tdf):\n",
    "  \"\"\"\n",
    "  when converting tdf seconds to TRs,\n",
    "    there are duplicates\n",
    "    to resolve duplicates, \n",
    "      take row with most non-zeros\n",
    "      *NB could be improved*\n",
    "  \"\"\"\n",
    "  L = []\n",
    "  for idx in tdf.index.unique():\n",
    "    rows = tdf.loc[idx,:]\n",
    "    # detect if there are duplicates\n",
    "    if len(rows.shape)==1:\n",
    "      row = rows\n",
    "    elif len(rows.shape)>1:\n",
    "      # resolve duplicate entries\n",
    "      keep_row_num = np.sum(rows != 0,1).argmax()    \n",
    "      row = rows.iloc[keep_row_num]\n",
    "    L.append(row)\n",
    "  return pd.concat(L,1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_recall_transcription(recall_df,transcript_df):\n",
    "  \"\"\" \n",
    "  using wedding_id to match between \n",
    "  information in log_df with transcriptions\n",
    "  \"\"\"\n",
    "  for idx,recall_df_row in recall_df.iterrows():\n",
    "    transcript_df_row_num = recall_df_row.TR - int(recall_df_row.onsetTR)\n",
    "    recall = transcript_df.loc[transcript_df_row_num,\"W%i\"%int(recall_df_row.wed_id)]\n",
    "    recall_df.loc[idx,'recall'] = int(recall)\n",
    "  recall_df = recall_df.astype({'recall':int})\n",
    "  return recall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### schema and path information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible indexer with Series",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b101498c741d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#   recall_df.loc[idx,'wed_schema'] = sub_wed_view_row.wed_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mrecall_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrecall_df_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wed_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrecall_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/brain/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/brain/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1599\u001b[0m                             \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m                         )\n\u001b[0;32m-> 1601\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m                         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/brain/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_align_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0minfo_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minfo_axis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/brain/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_align_series\u001b[0;34m(self, indexer, ser, multiindex_indexer)\u001b[0m\n\u001b[1;32m   1952\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1954\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Incompatible indexer with Series\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_align_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible indexer with Series"
     ]
    }
   ],
   "source": [
    "sub_num = 33\n",
    "recall_df = build_sub_recall_df(sub_num)\n",
    "\n",
    "for idx,recall_df_row in recall_df.iterrows():\n",
    "  ## select rows corresponding to subject/wedding\n",
    "  sub_bool = (view_df.sub_num == recall_df_row.sub_num)\n",
    "  wed_bool = (view_df.wed_id == recall_df_row.wed_id)\n",
    "  sub_wed_view_df = view_df[sub_bool & wed_bool]\n",
    "  ## get row corresponding to first event\n",
    "  sub_wed_view_row = sub_wed_view_df[sub_wed_view_df.state.str[0] == '2']\n",
    "  path = sub_wed_view_row.wed_schema + sub_wed_view_row.state.str[1]\n",
    "#   recall_df.loc[idx,'wed_schema'] = sub_wed_view_row.wed_schema\n",
    "  print(idx\n",
    "  recall_df.loc[recall_df_row,'wed_path'] = path\n",
    "  \n",
    "recall_df\n",
    "\n",
    "#   # find path for given sub/wed in view_df\n",
    "#   view_row = sub_wed_view_df[sub_wed_view_df.state.str[0] == '2']\n",
    "#   path = view_row.schema + view_row.state.str[1]\n",
    "#   # include path in recall_df\n",
    "#   try:\n",
    "#     recall_df.loc[idx,'schema'] = str(view_row.schema.values[0])\n",
    "#     recall_df.loc[idx,'path'] = str(path.values[0])\n",
    "#   except:\n",
    "#     print('err S=',row.sub_num)\n",
    "    \n",
    "# sub_wed_view_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop over subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sub_recall_df(sub_num):\n",
    "  log_df = read_mylogfile(sub_num)\n",
    "  log_df = include_offsetTR_col(log_df)\n",
    "  recall_df = init_recall_df(log_df)\n",
    "  ## include transcribed recall\n",
    "  transcript_df = load_transcript_df(sub_num)\n",
    "  transcript_df = resolve_tdf_duplicates(transcript_df)\n",
    "  recall_df = include_recall_transcription(recall_df,transcript_df)\n",
    "  return recall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TR</th>\n",
       "      <th>sub_num</th>\n",
       "      <th>wed_id</th>\n",
       "      <th>wed_num_recall</th>\n",
       "      <th>onsetTR</th>\n",
       "      <th>offsetTR</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>504</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>483</td>\n",
       "      <td>509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>505</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>483</td>\n",
       "      <td>509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>506</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>483</td>\n",
       "      <td>509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>507</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>483</td>\n",
       "      <td>509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>508</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>483</td>\n",
       "      <td>509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1033 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TR  sub_num wed_id  wed_num_recall  onsetTR  offsetTR  recall\n",
       "0       0       33     22               0        0        40       0\n",
       "1       1       33     22               0        0        40       0\n",
       "2       2       33     22               0        0        40       0\n",
       "3       3       33     22               0        0        40       0\n",
       "4       4       33     22               0        0        40       1\n",
       "...   ...      ...    ...             ...      ...       ...     ...\n",
       "1028  504       34     17              11      483       509       0\n",
       "1029  505       34     17              11      483       509       0\n",
       "1030  506       34     17              11      483       509       0\n",
       "1031  507       34     17              11      483       509       0\n",
       "1032  508       34     17              11      483       509       0\n",
       "\n",
       "[1033 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_num=33\n",
    "L = []\n",
    "for sub_num in np.arange(33,35):\n",
    "  sub_recall_df = build_sub_recall_df(sub_num)\n",
    "  L.append(sub_recall_df)\n",
    "  \n",
    "group_recall_df = pd.concat(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean-up and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_recall_df.index = np.arange(len(group_recall_df))\n",
    "group_recall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" loop over subejcts to make group recall_df\"\"\"\n",
    "rm_subs = [2,15]\n",
    "\n",
    "L = []\n",
    "err_sub_L = []\n",
    "for sub_num in np.arange(45):\n",
    "  if sub_num in rm_subs:\n",
    "    continue\n",
    "  print('sub',sub_num)\n",
    "  try:\n",
    "    ldf = load_logdf(sub_num)\n",
    "    tdf = load_transcript_df(sub_num)\n",
    "    sub_recall_df = make_sub_recall_df(ldf,tdf)\n",
    "    L.append(sub_recall_df)\n",
    "  except:\n",
    "    err_sub_L.append(sub_num)\n",
    "    print('ERR. sub',sub_num)\n",
    "\n",
    "## CONCAT AND REIDNEX\n",
    "recall_df = pd.concat(L)\n",
    "recall_df.index = np.arange(len(recall_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'missing subjects'\n",
    "err_sub_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### include path and schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recall_df = recall_df.astype({'TR':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_df.to_csv('deriv/recall_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
